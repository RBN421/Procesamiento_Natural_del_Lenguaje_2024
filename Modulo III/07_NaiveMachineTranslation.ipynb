{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RuHVvSW0ovy"
      },
      "source": [
        "<img style=\"float: left;;\" src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/alinco.png?raw=1' /></a>\n",
        "\n",
        "# Modulo II: Traducción automática y Local Sensitive Hashing (LSH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlC_7mGo0ovz"
      },
      "source": [
        "Ahora implementaremos un sistema de traducción automática y luego\n",
        "veremos cómo funcionan las funciones hash. Empecemos importando\n",
        "las funciones requeridas!\n",
        "\n",
        "\n",
        "```\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ozhQJhsf0ovz"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "import time\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from utils import (cosine_similarity, get_dict,\n",
        "                   process_tweet)\n",
        "from os import getcwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "71i00mcU0ov0"
      },
      "outputs": [],
      "source": [
        "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "id": "K2DGJXZk1C7c",
        "outputId": "a6d925b0-b74a-48fe-db14-ee835ffc78aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgEzMxci0ov0"
      },
      "source": [
        "# Embeddings para palabras en Inglés y Frances\n",
        "\n",
        "Escribe un programa que traduzca del inglés al francés.\n",
        "\n",
        "## Los datos\n",
        "\n",
        "El conjunto de datos completo para las wordembeddings en inglés es de aproximadamente 3,64 gigabytes, y el francés\n",
        "son de aproximadamente 629 megabytes. Trabajaremos con un conjunto más pequeño de datos, una muestra significativa.\n",
        "\n",
        "* English embeddings from Google code archive word2vec\n",
        "[look for GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
        "    * You'll need to unzip the file first.\n",
        "* and the French embeddings from\n",
        "[cross_lingual_text_classification](https://github.com/vjstark/crosslingual_text_classification).\n",
        "    * in the terminal, type (in one line)\n",
        "    `curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec`\n",
        "\n",
        "copiar y pegar el código para descargar todos los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l26TB9HG0ov0"
      },
      "source": [
        "```python\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
        "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n",
        "\n",
        "\n",
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict('en-fr.train.txt')\n",
        "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict('en-fr.test.txt')\n",
        "print('The length of the english to french test dictionary is', len(en_fr_train))\n",
        "\n",
        "english_set = set(en_embeddings.vocab)\n",
        "french_set = set(fr_embeddings.vocab)\n",
        "en_embeddings_subset = {}\n",
        "fr_embeddings_subset = {}\n",
        "french_words = set(en_fr_train.values())\n",
        "\n",
        "for en_word in en_fr_train.keys():\n",
        "    fr_word = en_fr_train[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "for en_word in en_fr_test.keys():\n",
        "    fr_word = en_fr_test[en_word]\n",
        "    if fr_word in french_set and en_word in english_set:\n",
        "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
        "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
        "\n",
        "\n",
        "pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\n",
        "pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWiNj4w00ov0"
      },
      "source": [
        "#### Un subconjunto de datos\n",
        "\n",
        "Para realizar esta actividad, usaremos un subconjunto de datos que contenga algunos word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d7tzzRiy0ov0"
      },
      "outputs": [],
      "source": [
        "en_embeddings_subset = pickle.load(open('en_embeddings.p', 'rb'))\n",
        "fr_embeddings_subset = pickle.load(open('fr_embeddings.p', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(en_embeddings_subset)"
      ],
      "metadata": {
        "id": "6zaG2kRp2LYQ",
        "outputId": "cc1b13f0-ebda-4234-f659-a2dff1bc96e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_embeddings_subset['king'])"
      ],
      "metadata": {
        "id": "mU3yyDsU2PR-",
        "outputId": "30f2a7c5-2f6e-49e2-9d4b-026eae79ca00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(fr_embeddings_subset['génie'])"
      ],
      "metadata": {
        "id": "VaqVHkj02UdL",
        "outputId": "49f8b49d-01fb-4652-b6ac-4ff13d74d663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uykNreyJ0ov0"
      },
      "source": [
        "#### Observando los datos\n",
        "\n",
        "* en_embeddings_subset: el key es una palabra en inglés, y el valor es un arreglo de 300 elementos, que representa el wordembedding de la palabra.\n",
        "```\n",
        "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
        "```\n",
        "\n",
        "* fr_embeddings_subset: el key es una palabra en francés,y el valor es un arreglo de 300 elementos, que representa el wordembedding de la palabra.\n",
        "```\n",
        "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Fp412z0ov0"
      },
      "source": [
        "#### Cargue dos diccionarios que mapean las palabras del inglés al francés\n",
        "* Un diccionario de entrenamiento\n",
        "* y un diccionario de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "13f4Dsfj0ov1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def get_dic(file_name):\n",
        "  my_file = pd.read_csv(file_name, delimiter='')\n",
        "  etof = {}\n",
        "  for i in range(len(my_file)):\n",
        "    en = my_file.loc[i][0]\n",
        "    fr = my_file.loc[i][1]\n",
        "    etof[en] = fr\n",
        "  return etof\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LlvL8QRH0ov1"
      },
      "outputs": [],
      "source": [
        "#Cargar los diccionarios de inglés a francés\n",
        "en_fr_train = get_dict('en-fr.train.txt')\n",
        "en_fr_test = get_dict('en-fr.test.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_fr_train"
      ],
      "metadata": {
        "id": "YMJJu-ls3mOc",
        "outputId": "03da0544-8e12-4411-e0f2-bbf2f22e36e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 'la',\n",
              " 'and': 'et',\n",
              " 'was': 'était',\n",
              " 'for': 'pour',\n",
              " 'that': 'cela',\n",
              " 'with': 'avec',\n",
              " 'from': 'depuis',\n",
              " 'this': 'ce',\n",
              " 'utc': 'tuc',\n",
              " 'his': 'son',\n",
              " 'not': 'pas',\n",
              " 'are': 'sont',\n",
              " 'talk': 'parlez',\n",
              " 'which': 'lequel',\n",
              " 'also': 'egalement',\n",
              " 'were': 'étaient',\n",
              " 'but': 'mais',\n",
              " 'have': 'ont',\n",
              " 'one': 'one',\n",
              " 'new': 'nouveautés',\n",
              " 'first': 'premiers',\n",
              " 'page': 'page',\n",
              " 'you': 'you',\n",
              " 'they': 'eux',\n",
              " 'had': 'avais',\n",
              " 'article': 'article',\n",
              " 'who': 'who',\n",
              " 'all': 'all',\n",
              " 'their': 'leurs',\n",
              " 'there': 'là',\n",
              " 'made': 'fabriqué',\n",
              " 'its': 'son',\n",
              " 'people': 'personnes',\n",
              " 'may': 'peut',\n",
              " 'after': 'aprés',\n",
              " 'other': 'autres',\n",
              " 'should': 'devrais',\n",
              " 'two': 'deux',\n",
              " 'score': 'partition',\n",
              " 'her': 'her',\n",
              " 'can': 'peut',\n",
              " 'would': 'ferait',\n",
              " 'more': 'plus',\n",
              " 'she': 'elle',\n",
              " 'when': 'quand',\n",
              " 'time': 'heure',\n",
              " 'team': 'equipe',\n",
              " 'american': 'américains',\n",
              " 'such': 'telles',\n",
              " 'discussion': 'débat',\n",
              " 'links': 'liens',\n",
              " 'only': 'seule',\n",
              " 'some': 'quelques',\n",
              " 'see': 'vois',\n",
              " 'united': 'unies',\n",
              " 'years': 'ans',\n",
              " 'school': 'école',\n",
              " 'world': 'mondiale',\n",
              " 'university': 'universitaire',\n",
              " 'during': 'lors',\n",
              " 'out': 'out',\n",
              " 'state': 'état',\n",
              " 'states': 'états',\n",
              " 'national': 'nationales',\n",
              " 'wikipedia': 'wikipedia',\n",
              " 'year': 'année',\n",
              " 'most': 'most',\n",
              " 'city': 'villes',\n",
              " 'used': 'utilisée',\n",
              " 'then': 'puis',\n",
              " 'county': 'comté',\n",
              " 'external': 'externes',\n",
              " 'where': 'où',\n",
              " 'will': 'sera',\n",
              " 'what': 'quelle',\n",
              " 'delete': 'effacer',\n",
              " 'these': 'ces',\n",
              " 'january': 'janvier',\n",
              " 'march': 'mars',\n",
              " 'august': 'août',\n",
              " 'july': 'juillet',\n",
              " 'being': 'être',\n",
              " 'film': 'film',\n",
              " 'him': 'lui',\n",
              " 'many': 'plusieurs',\n",
              " 'south': 'sud',\n",
              " 'september': 'septembre',\n",
              " 'like': 'aimez',\n",
              " 'between': 'entre',\n",
              " 'october': 'octobre',\n",
              " 'three': 'three',\n",
              " 'june': 'juin',\n",
              " 'well': 'bah',\n",
              " 'use': 'utilisez',\n",
              " 'war': 'war',\n",
              " 'under': 'under',\n",
              " 'them': 'eux',\n",
              " 'april': 'avril',\n",
              " 'born': 'born',\n",
              " 'december': 'decembre',\n",
              " 'link': 'lien',\n",
              " 'later': 'ultérieur',\n",
              " 'part': 'partie',\n",
              " 'november': 'novembre',\n",
              " 'players': 'joueurs',\n",
              " 'list': 'listes',\n",
              " 'please': 'svp',\n",
              " 'following': 'suivant',\n",
              " 'february': 'février',\n",
              " 'known': 'connu',\n",
              " 'second': 'seconde',\n",
              " 'name': 'noms',\n",
              " 'group': 'groupe',\n",
              " 'history': 'historique',\n",
              " 'series': 'séries',\n",
              " 'just': 'juste',\n",
              " 'north': 'nord',\n",
              " 'work': 'travailler',\n",
              " 'before': 'avant',\n",
              " 'since': 'puisque',\n",
              " 'season': 'saisons',\n",
              " 'both': 'both',\n",
              " 'high': 'élevé',\n",
              " 'through': 'via',\n",
              " 'district': 'district',\n",
              " 'now': 'maintenant',\n",
              " 'comments': 'observations',\n",
              " 'because': 'parceque',\n",
              " 'football': 'football',\n",
              " 'music': 'musique',\n",
              " 'however': 'cependant',\n",
              " 'diff': 'diff',\n",
              " 'century': 'century',\n",
              " 'league': 'ligue',\n",
              " 'edits': 'modifications',\n",
              " 'debate': 'débat',\n",
              " 'title': 'titre',\n",
              " 'articles': 'articles',\n",
              " 'john': 'john',\n",
              " 'same': 'même',\n",
              " 'including': 'comprenant',\n",
              " 'could': 'pourraient',\n",
              " 'english': 'anglais',\n",
              " 'album': 'album',\n",
              " 'number': 'numéro',\n",
              " 'against': 'against',\n",
              " 'family': 'familles',\n",
              " 'user': 'usager',\n",
              " 'based': 'basé',\n",
              " 'area': 'domaine',\n",
              " 'became': 'devint',\n",
              " 'york': 'york',\n",
              " 'life': 'vie',\n",
              " 'british': 'britannique',\n",
              " 'international': 'internationale',\n",
              " 'game': 'jeu',\n",
              " 'club': 'club',\n",
              " 'your': 'vos',\n",
              " 'early': 'tôt',\n",
              " 'best': 'meilleurs',\n",
              " 'west': 'west',\n",
              " 'house': 'maison',\n",
              " 'company': 'société',\n",
              " 'general': 'général',\n",
              " 'left': 'gauche',\n",
              " 'very': 'trés',\n",
              " 'here': 'voici',\n",
              " 'don': 'don',\n",
              " 'living': 'vivre',\n",
              " 'day': 'journee',\n",
              " 'several': 'plusieurs',\n",
              " 'place': 'lieu',\n",
              " 'party': 'fête',\n",
              " 'college': 'université',\n",
              " 'result': 'résultat',\n",
              " 'keep': 'conserver',\n",
              " 'appropriate': 'appropriée',\n",
              " 'four': 'quatre',\n",
              " 'even': 'même',\n",
              " 'class': 'classe',\n",
              " 'government': 'gouvernements',\n",
              " 'how': 'comment',\n",
              " 'called': 'appelés',\n",
              " 'did': 'did',\n",
              " 'each': 'chacun',\n",
              " 'found': 'trouvés',\n",
              " 'center': 'centre',\n",
              " 'per': 'per',\n",
              " 'style': 'style',\n",
              " 'com': 'com',\n",
              " 'long': 'long',\n",
              " 'country': 'pays',\n",
              " 'back': 'revenir',\n",
              " 'way': 'way',\n",
              " 'www': 'www',\n",
              " 'modify': 'modifier',\n",
              " 'end': 'fin',\n",
              " 'make': 'faire',\n",
              " 'public': 'publique',\n",
              " 'played': 'joué',\n",
              " 'won': 'gagnés',\n",
              " 'another': 'another',\n",
              " 'released': 'libéré',\n",
              " 'added': 'ajoutée',\n",
              " 'support': 'appui',\n",
              " 'games': 'jeux',\n",
              " 'former': 'ancienne',\n",
              " 'those': 'ceux',\n",
              " 'films': 'films',\n",
              " 'church': 'eglise',\n",
              " 'east': 'orient',\n",
              " 'line': 'line',\n",
              " 'major': 'majeur',\n",
              " 'members': 'adhérents',\n",
              " 'good': 'bonnes',\n",
              " 'much': 'beaucoup',\n",
              " 'image': 'image',\n",
              " 'show': 'spectacle',\n",
              " 'still': 'toujours',\n",
              " 'think': 'réfléchir',\n",
              " 'below': 'dessous',\n",
              " 'town': 'ville',\n",
              " 'last': 'dernières',\n",
              " 'system': 'système',\n",
              " 'right': 'droit',\n",
              " 'song': 'chanson',\n",
              " 'notable': 'remarquables',\n",
              " 'section': 'section',\n",
              " 'single': 'célibataires',\n",
              " 'included': 'compris',\n",
              " 'align': 'alignement',\n",
              " 'home': 'accueil',\n",
              " 'women': 'femme',\n",
              " 'television': 'télé',\n",
              " 'seed': 'semence',\n",
              " 'member': 'membre',\n",
              " 'goals': 'objectifs',\n",
              " 'sources': 'sources',\n",
              " 'book': 'réserver',\n",
              " 'station': 'gare',\n",
              " 'order': 'commander',\n",
              " 'old': 'vieille',\n",
              " 'information': 'information',\n",
              " 'set': 'définir',\n",
              " 'own': 'posséder',\n",
              " 'text': 'texte',\n",
              " 'band': 'bande',\n",
              " 'point': 'point',\n",
              " 'local': 'locaux',\n",
              " 'around': 'alentour',\n",
              " 'river': 'rivière',\n",
              " 'top': 'haut',\n",
              " 'main': 'principaux',\n",
              " 'language': 'langues',\n",
              " 'french': 'françaises',\n",
              " 'https': 'https',\n",
              " 'named': 'nommés',\n",
              " 'off': 'hors',\n",
              " 'note': 'notez',\n",
              " 'career': 'carrière',\n",
              " 'original': 'originaux',\n",
              " 'age': 'âges',\n",
              " 'service': 'service',\n",
              " 'established': 'établis',\n",
              " 'located': 'situé',\n",
              " 'said': 'disait',\n",
              " 'website': 'site',\n",
              " 'population': 'population',\n",
              " 'air': 'air',\n",
              " 'german': 'allemande',\n",
              " 'law': 'droit',\n",
              " 'military': 'militaires',\n",
              " 'great': 'grand',\n",
              " 'clubs': 'clubs',\n",
              " 'published': 'publié',\n",
              " 'president': 'président',\n",
              " 'park': 'parc',\n",
              " 'official': 'officiel',\n",
              " 'case': 'affaire',\n",
              " 'london': 'londres',\n",
              " 'times': 'fois',\n",
              " 'although': 'quoique',\n",
              " 'small': 'petite',\n",
              " 'third': 'troisièmement',\n",
              " 'different': 'différent',\n",
              " 'due': 'dû',\n",
              " 'get': 'obtenir',\n",
              " 'village': 'village',\n",
              " 'closed': 'clos',\n",
              " 'art': 'artistique',\n",
              " 'player': 'lecteur',\n",
              " 'final': 'définitive',\n",
              " 'community': 'collectivité',\n",
              " 'held': 'tenu',\n",
              " 'again': 'encore',\n",
              " 'began': 'commencé',\n",
              " 'army': 'armée',\n",
              " 'award': 'récompense',\n",
              " 'without': 'sans',\n",
              " 'death': 'mort',\n",
              " 'built': 'construits',\n",
              " 'men': 'homme',\n",
              " 'large': 'grand',\n",
              " 'site': 'site',\n",
              " 'using': 'utilisant',\n",
              " 'deletion': 'suppression',\n",
              " 'white': 'blanc',\n",
              " 'five': 'cinq',\n",
              " 'central': 'centrale',\n",
              " 'road': 'chemin',\n",
              " 'children': 'enfant',\n",
              " 'free': 'libre',\n",
              " 'took': 'prit',\n",
              " 'england': 'angleterre',\n",
              " 'include': 'inclut',\n",
              " 'association': 'association',\n",
              " 'down': 'descendre',\n",
              " 'given': 'donnés',\n",
              " 'source': 'sources',\n",
              " 'california': 'californienne',\n",
              " 'man': 'homme',\n",
              " 'version': 'version',\n",
              " 'written': 'écrits',\n",
              " 'created': 'créés',\n",
              " 'media': 'médias',\n",
              " 'black': 'noirs',\n",
              " 'php': 'php',\n",
              " 'report': 'rapport',\n",
              " 'building': 'bâtiment',\n",
              " 'take': 'prends',\n",
              " 'division': 'circonscription',\n",
              " 'comment': 'commenter',\n",
              " 'having': 'avoir',\n",
              " 'king': 'king',\n",
              " 'edit': 'éditer',\n",
              " 'stadium': 'stadium',\n",
              " 'died': 'mort',\n",
              " 'ship': 'vaisseau',\n",
              " 'research': 'recherche',\n",
              " 'record': 'enregistrer',\n",
              " 'archive': 'archives',\n",
              " 'places': 'lieux',\n",
              " 'undo': 'défaire',\n",
              " 'cup': 'coupe',\n",
              " 'records': 'records',\n",
              " 'often': 'souvent',\n",
              " 'few': 'peu',\n",
              " 'received': 'reçue',\n",
              " 'side': 'latéral',\n",
              " 'power': 'pouvoir',\n",
              " 'education': 'éducation',\n",
              " 'know': 'savoir',\n",
              " 'category': 'catégories',\n",
              " 'water': 'eau',\n",
              " 'species': 'espèce',\n",
              " 'field': 'domaine',\n",
              " 'near': 'près',\n",
              " 'australia': 'australie',\n",
              " 'video': 'video',\n",
              " 'need': 'besoin',\n",
              " 'island': 'île',\n",
              " 'form': 'formulaire',\n",
              " 'find': 'trouvez',\n",
              " 'served': 'desservis',\n",
              " 'play': 'jouez',\n",
              " 'project': 'projet',\n",
              " 'radio': 'radio',\n",
              " 'works': 'oeuvres',\n",
              " 'proposed': 'proposés',\n",
              " 'every': 'chaque',\n",
              " 'development': 'développement',\n",
              " 'example': 'exemple',\n",
              " 'live': 'vivre',\n",
              " 'union': 'syndicat',\n",
              " 'india': 'indes',\n",
              " 'next': 'next',\n",
              " 'special': 'spéciale',\n",
              " 'court': 'cour',\n",
              " 'region': 'région',\n",
              " 'little': 'petite',\n",
              " 'short': 'courtes',\n",
              " 'william': 'william',\n",
              " 'province': 'province',\n",
              " 'western': 'western',\n",
              " 'son': 'fiston',\n",
              " 'france': 'france',\n",
              " 'council': 'conseil',\n",
              " 'others': 'autres',\n",
              " 'royal': 'royaux',\n",
              " 'current': 'actuelle',\n",
              " 'street': 'rue',\n",
              " 'full': 'complet',\n",
              " 'red': 'rouge',\n",
              " 'too': 'too',\n",
              " 'department': 'département',\n",
              " 'san': 'san',\n",
              " 'help': 'aide',\n",
              " 'among': 'parmi',\n",
              " 'preserved': 'préservées',\n",
              " 'james': 'james',\n",
              " 'open': 'ouvrir',\n",
              " 'force': 'forcer',\n",
              " 'position': 'position',\n",
              " 'head': 'têtes',\n",
              " 'director': 'réalisateur',\n",
              " 'father': 'père',\n",
              " 'track': 'morceau',\n",
              " 'http': 'http',\n",
              " 'canada': 'canada',\n",
              " 'never': 'never',\n",
              " 'australian': 'australie',\n",
              " 'george': 'george',\n",
              " 'jpg': 'jpg',\n",
              " 'level': 'niveau',\n",
              " 'late': 'tard',\n",
              " 'summer': 'été',\n",
              " 'society': 'société',\n",
              " 'moved': 'déplacée',\n",
              " 'office': 'bureau',\n",
              " 'period': 'période',\n",
              " 'championship': 'championnat',\n",
              " 'round': 'ronds',\n",
              " 'story': 'récit',\n",
              " 'songs': 'chansons',\n",
              " 'various': 'diverses',\n",
              " 'file': 'fichier',\n",
              " 'days': 'journées',\n",
              " 'land': 'terres',\n",
              " 'business': 'entreprises',\n",
              " 'reason': 'raison',\n",
              " 'america': 'amérique',\n",
              " 'million': 'millions',\n",
              " 'european': 'européen',\n",
              " 'term': 'terme',\n",
              " 'six': 'six',\n",
              " 'post': 'publication',\n",
              " 'why': 'why',\n",
              " 'produced': 'produites',\n",
              " 'subject': 'sujet',\n",
              " 'young': 'jeune',\n",
              " 'total': 'totaux',\n",
              " 'david': 'david',\n",
              " 'science': 'sciences',\n",
              " 'related': 'liés',\n",
              " 'rock': 'rock',\n",
              " 'archived': 'archivés',\n",
              " 'railway': 'ferroviaire',\n",
              " 'become': 'devenir',\n",
              " 'led': 'led',\n",
              " 'students': 'élèves',\n",
              " 'started': 'commencée',\n",
              " 'news': 'actualités',\n",
              " 'described': 'décrite',\n",
              " 'role': 'rôle',\n",
              " 'election': 'élections',\n",
              " 'albums': 'albums',\n",
              " 'present': 'présenter',\n",
              " 'indian': 'indien',\n",
              " 'kingdom': 'royaume',\n",
              " 'books': 'livres',\n",
              " 'important': 'importants',\n",
              " 'northern': 'nord',\n",
              " 'love': 'love',\n",
              " 'run': 'exécuter',\n",
              " 'canadian': 'canadien',\n",
              " 'press': 'presse',\n",
              " 'rather': 'plutôt',\n",
              " 'type': 'tapez',\n",
              " 'act': 'act',\n",
              " 'editor': 'editeur',\n",
              " 'came': 'vint',\n",
              " 'schools': 'écoles',\n",
              " 'program': 'programme',\n",
              " 'once': 'once',\n",
              " 'social': 'social',\n",
              " 'germany': 'allemagne',\n",
              " 'production': 'production',\n",
              " 'male': 'homme',\n",
              " 'might': 'pourrait',\n",
              " 'awards': 'récompenses',\n",
              " 'points': 'points',\n",
              " 'similar': 'semblable',\n",
              " 'professional': 'professionnelles',\n",
              " 'say': 'dis',\n",
              " 'background': 'contexte',\n",
              " 'enough': 'assez',\n",
              " 'lead': 'plomb',\n",
              " 'either': 'soit',\n",
              " 'common': 'commun',\n",
              " 'overlap': 'chevauchements',\n",
              " 'data': 'données',\n",
              " 'color': 'couleurs',\n",
              " 'better': 'meilleur',\n",
              " 'person': 'personne',\n",
              " 'services': 'services',\n",
              " 'bgcolor': 'bgcolor',\n",
              " 'museum': 'musée',\n",
              " 'battle': 'combat',\n",
              " 'went': 'allé',\n",
              " 'sports': 'sports',\n",
              " 'already': 'dejà',\n",
              " 'currently': 'présentement',\n",
              " 'hall': 'hall',\n",
              " 'buildings': 'édifices',\n",
              " 'historic': 'historiques',\n",
              " 'date': 'date',\n",
              " 'deleted': 'supprimées',\n",
              " 'considered': 'considérés',\n",
              " 'change': 'changement',\n",
              " 'location': 'lieu',\n",
              " 'seems': 'semble',\n",
              " 'must': 'devez',\n",
              " 'yes': 'yes',\n",
              " 'our': 'nos',\n",
              " 'southern': 'méridionale',\n",
              " 'lost': 'perdus',\n",
              " 'something': 'quelquechose',\n",
              " 'review': 'révision',\n",
              " 'together': 'together',\n",
              " 'robert': 'thierry',\n",
              " 'less': 'moins',\n",
              " 'japanese': 'japonaises',\n",
              " 'groups': 'groupes',\n",
              " 'content': 'contenus',\n",
              " 'involved': 'impliquée',\n",
              " 'isbn': 'isbn',\n",
              " 'board': 'planche',\n",
              " 'japan': 'japon',\n",
              " 'control': 'contrôle',\n",
              " 'policy': 'politique',\n",
              " 'modern': 'modernes',\n",
              " 'human': 'humain',\n",
              " 'half': 'demi',\n",
              " 'design': 'conception',\n",
              " 'event': 'evénement',\n",
              " 'events': 'événements',\n",
              " 'available': 'disponibles',\n",
              " 'done': 'faite',\n",
              " 'washington': 'washington',\n",
              " 'real': 'vraie',\n",
              " 'start': 'début',\n",
              " 'personal': 'personnels',\n",
              " 'action': 'action',\n",
              " 'space': 'espace',\n",
              " 'areas': 'zones',\n",
              " 'star': 'étoiles',\n",
              " 'really': 'réellement',\n",
              " 'china': 'chine',\n",
              " 'possible': 'possibles',\n",
              " 'paul': 'paul',\n",
              " 'working': 'travailler',\n",
              " 'taken': 'prise',\n",
              " 'far': 'loin',\n",
              " 'going': 'aller',\n",
              " 'minister': 'ministre',\n",
              " 'lake': 'lac',\n",
              " 'reported': 'signalée',\n",
              " 'popular': 'populaire',\n",
              " 'married': 'marié',\n",
              " 'founded': 'fondée',\n",
              " 'europe': 'europe',\n",
              " 'author': 'auteur',\n",
              " 'away': 'loin',\n",
              " 'independent': 'indépendants',\n",
              " 'process': 'procédé',\n",
              " 'teams': 'equipes',\n",
              " 'character': 'personnage',\n",
              " 'low': 'bas',\n",
              " 'michael': 'michel',\n",
              " 'pages': 'pages',\n",
              " 'light': 'léger',\n",
              " 'big': 'grand',\n",
              " 'seen': 'vus',\n",
              " 'release': 'libèrent',\n",
              " 'want': 'voulez',\n",
              " 'episode': 'episode',\n",
              " 'wrote': 'écrit',\n",
              " 'republic': 'république',\n",
              " 'thomas': 'thomas',\n",
              " 'companies': 'sociétés',\n",
              " 'via': 'via',\n",
              " 'russian': 'russe',\n",
              " 'thanks': 'remerciements',\n",
              " 'put': 'mettre',\n",
              " 'race': 'race',\n",
              " 'worked': 'travaillé',\n",
              " 'route': 'parcours',\n",
              " 'recorded': 'enregistré',\n",
              " 'someone': 'someone',\n",
              " 'civil': 'civiles',\n",
              " 'police': 'policier',\n",
              " 'charles': 'charles',\n",
              " 'listed': 'répertoriés',\n",
              " 'users': 'utilisateurs',\n",
              " 'template': 'modèle',\n",
              " 'eastern': 'oriental',\n",
              " 'body': 'corps',\n",
              " 'question': 'question',\n",
              " 'italian': 'italiens',\n",
              " 'featured': 'vedettes',\n",
              " 'week': 'semaines',\n",
              " 'editors': 'éditeurs',\n",
              " 'texas': 'texas',\n",
              " 'chief': 'chef',\n",
              " 'close': 'proche',\n",
              " 'match': 'match',\n",
              " 'roman': 'romains',\n",
              " 'come': 'venir',\n",
              " 'opened': 'ouvert',\n",
              " 'tour': 'tournée',\n",
              " 'sea': 'mer',\n",
              " 'cross': 'croisé',\n",
              " 'playing': 'jouant',\n",
              " 'health': 'santé',\n",
              " 'institute': 'institut',\n",
              " 'caps': 'bouchons',\n",
              " 'forces': 'forces',\n",
              " 'green': 'vertes',\n",
              " 'rights': 'droits',\n",
              " 'evidence': 'preuves',\n",
              " 'originally': 'initialement',\n",
              " 'aircraft': 'avions',\n",
              " 'arts': 'arts',\n",
              " 'range': 'portée',\n",
              " 'probably': 'sûrement',\n",
              " 'consensus': 'consensus',\n",
              " 'bar': 'barre',\n",
              " 'problem': 'problématique',\n",
              " 'look': 'regardes',\n",
              " 'issues': 'problèmes',\n",
              " 'alumni': 'anciens',\n",
              " 'average': 'moyenne',\n",
              " 'network': 'réseau',\n",
              " 'win': 'gagnant',\n",
              " 'shows': 'spectacles',\n",
              " 'wife': 'femme',\n",
              " 'returned': 'retournée',\n",
              " 'night': 'soir',\n",
              " 'magazine': 'magasine',\n",
              " 'centre': 'centre',\n",
              " 'joined': 'rejoint',\n",
              " 'usually': 'généralement',\n",
              " 'middle': 'milieu',\n",
              " 'completed': 'terminé',\n",
              " 'elected': 'élus',\n",
              " 'significant': 'significatifs',\n",
              " 'african': 'africaines',\n",
              " 'able': 'capable',\n",
              " 'google': 'google',\n",
              " 'stage': 'scène',\n",
              " 'addition': 'ajout',\n",
              " 'ireland': 'irlande',\n",
              " 'today': 'aujourdhui',\n",
              " 'academy': 'academy',\n",
              " 'saint': 'saint',\n",
              " 'self': 'self',\n",
              " 'itself': 'soi',\n",
              " 'continued': 'continué',\n",
              " 'stations': 'stations',\n",
              " 'mother': 'maman',\n",
              " 'appeared': 'semblait',\n",
              " 'africa': 'afrique',\n",
              " 'culture': 'culture',\n",
              " 'spanish': 'espagnols',\n",
              " 'grand': 'grand',\n",
              " 'committee': 'comité',\n",
              " 'things': 'choses',\n",
              " 'fire': 'incendies',\n",
              " 'changed': 'changée',\n",
              " 'gold': 'gold',\n",
              " 'female': 'femme',\n",
              " 'course': 'cours',\n",
              " 'directed': 'orienté',\n",
              " 'months': 'mois',\n",
              " 'chinese': 'chinoise',\n",
              " 'previous': 'précédents',\n",
              " 'developed': 'développé',\n",
              " 'size': 'tailles',\n",
              " 'mentioned': 'mentionnés',\n",
              " 'add': 'ajoutez',\n",
              " 'festival': 'fête',\n",
              " 'peter': 'pierre',\n",
              " 'basketball': 'basketball',\n",
              " 'move': 'déplacer',\n",
              " 'performance': 'rendement',\n",
              " 'standard': 'standard',\n",
              " 'means': 'signifie',\n",
              " 'give': 'donnez',\n",
              " 'training': 'entraînement',\n",
              " 'artist': 'artistes',\n",
              " 'word': 'mot',\n",
              " 'blue': 'bleues',\n",
              " 'primary': 'primaires',\n",
              " 'announced': 'annoncée',\n",
              " 'value': 'valeur',\n",
              " 'christian': 'chrétien',\n",
              " 'private': 'privés',\n",
              " 'catholic': 'catholiques',\n",
              " 'artists': 'artistes',\n",
              " 'includes': 'comprend',\n",
              " 'view': 'visualiser',\n",
              " 'thus': 'ainsi',\n",
              " 'almost': 'quasiment',\n",
              " 'baseball': 'baseball',\n",
              " 'seven': 'sept',\n",
              " 'appears': 'apparait',\n",
              " 'ever': 'ever',\n",
              " 'provide': 'fournissent',\n",
              " 'technology': 'technologie',\n",
              " 'olympics': 'olympiques',\n",
              " 'future': 'avenir',\n",
              " 'formed': 'formée',\n",
              " 'census': 'recensement',\n",
              " 'images': 'images',\n",
              " 'los': 'los',\n",
              " 'results': 'résultats',\n",
              " 'return': 'revenir',\n",
              " 'quality': 'qualité',\n",
              " 'construction': 'construction',\n",
              " 'zealand': 'zélande',\n",
              " 'front': 'devant',\n",
              " 'cover': 'housse',\n",
              " 'model': 'maquette',\n",
              " 'despite': 'malgré',\n",
              " 'read': 'lis',\n",
              " 'material': 'matériau',\n",
              " 'strong': 'fort',\n",
              " 'coach': 'entraîneur',\n",
              " 'henry': 'henry',\n",
              " 'footballers': 'footballeurs',\n",
              " 'mark': 'mark',\n",
              " 'rev': 'rev',\n",
              " 'organization': 'organisation',\n",
              " 'studies': 'études',\n",
              " 'federal': 'fédérales',\n",
              " 'richard': 'richard',\n",
              " 'html': 'html',\n",
              " 'virginia': 'virginia',\n",
              " 'car': 'voiture',\n",
              " 'attack': 'attaquer',\n",
              " 'conference': 'conférence',\n",
              " 'outside': 'exterieur',\n",
              " 'study': 'étude',\n",
              " 'brother': 'frere',\n",
              " 'names': 'noms',\n",
              " 'writer': 'scénariste',\n",
              " 'characters': 'caractères',\n",
              " 'musical': 'musical',\n",
              " 'nothing': 'rien',\n",
              " 'border': 'bordure',\n",
              " 'medical': 'médical',\n",
              " 'countries': 'pays',\n",
              " 'past': 'passés',\n",
              " 'writing': 'écrire',\n",
              " 'makes': 'rend',\n",
              " 'interest': 'intérêt',\n",
              " 'provided': 'fournis',\n",
              " 'killed': 'tués',\n",
              " 'medal': 'médailles',\n",
              " 'signed': 'signés',\n",
              " 'label': 'étiquettes',\n",
              " 'fair': 'équitables',\n",
              " 'search': 'recherchez',\n",
              " 'bay': 'bay',\n",
              " 'reference': 'référence',\n",
              " 'especially': 'spécialement',\n",
              " 'removed': 'supprimé',\n",
              " 'library': 'librairie',\n",
              " 'eventually': 'finalement',\n",
              " 'management': 'gestion',\n",
              " 'references': 'références',\n",
              " 'features': 'fonctionnalités',\n",
              " 'navy': 'marine',\n",
              " 'guitar': 'guitares',\n",
              " 'hill': 'colline',\n",
              " 'sure': 'sûr',\n",
              " 'historical': 'historiques',\n",
              " 'lower': 'inférieure',\n",
              " 'daughter': 'fille',\n",
              " 'appointed': 'nommés',\n",
              " 'reading': 'lire',\n",
              " 'yet': 'pourtant',\n",
              " 'systems': 'systèmes',\n",
              " 'debut': 'débuts',\n",
              " 'movement': 'mouvement',\n",
              " 'specific': 'spécifique',\n",
              " 'always': 'toujours',\n",
              " 'actor': 'acteur',\n",
              " 'natural': 'naturelle',\n",
              " 'clear': 'effacer',\n",
              " 'coast': 'côte',\n",
              " 'let': 'let',\n",
              " 'got': 'got',\n",
              " 'chicago': 'chicago',\n",
              " 'championships': 'championnats',\n",
              " 'pennsylvania': 'pennsylvanie',\n",
              " 'ten': 'ten',\n",
              " 'performed': 'effectué',\n",
              " 'individual': 'individuel',\n",
              " 'designed': 'conçus',\n",
              " 'rule': 'règle',\n",
              " 'etc': 'etc',\n",
              " 'lists': 'listes',\n",
              " 'paris': 'paris',\n",
              " 'thought': 'pensée',\n",
              " 'brown': 'brune',\n",
              " 'hand': 'hand',\n",
              " 'needs': 'besoins',\n",
              " 'reliable': 'fiables',\n",
              " 'smith': 'forgeron',\n",
              " 'generally': 'généralement',\n",
              " 'base': 'base',\n",
              " 'sometimes': 'parfois',\n",
              " 'florida': 'floride',\n",
              " 'capital': 'capital',\n",
              " 'valley': 'vallée',\n",
              " 'bank': 'banque',\n",
              " 'ground': 'moulu',\n",
              " 'reached': 'atteint',\n",
              " 'italy': 'italie',\n",
              " 'energy': 'energie',\n",
              " 'believe': 'crois',\n",
              " 'leader': 'leader',\n",
              " 'active': 'actifs',\n",
              " 'online': 'online',\n",
              " 'block': 'blocage',\n",
              " 'bridge': 'passerelle',\n",
              " 'families': 'familles',\n",
              " 'changes': 'changements',\n",
              " 'followed': 'suivies',\n",
              " 'industry': 'industrie',\n",
              " 'collection': 'collecte',\n",
              " 'request': 'demandez',\n",
              " 'soon': 'bientot',\n",
              " 'olympic': 'olympiques',\n",
              " 'sold': 'vendu',\n",
              " 'writers': 'écrivains',\n",
              " 'professor': 'professeure',\n",
              " 'studio': 'studio',\n",
              " 'mexico': 'mexique',\n",
              " 'competition': 'concours',\n",
              " 'campaign': 'campagne',\n",
              " 'org': 'org',\n",
              " 'theatre': 'théâtres',\n",
              " 'particular': 'particulier',\n",
              " 'empire': 'empire',\n",
              " 'length': 'longueurs',\n",
              " 'islands': 'iles',\n",
              " 'singer': 'chanteuse',\n",
              " 'create': 'créent',\n",
              " 'redirect': 'réorienter',\n",
              " 'additional': 'additionnel',\n",
              " 'soviet': 'soviétiques',\n",
              " 'market': 'marché',\n",
              " 'words': 'mots',\n",
              " 'producer': 'producteurs',\n",
              " 'notes': 'notes',\n",
              " 'hockey': 'hockey',\n",
              " 'code': 'code',\n",
              " 'referee': 'arbitre',\n",
              " 'fourth': 'quatrièmement',\n",
              " 'sport': 'sport',\n",
              " 'van': 'van',\n",
              " 'mary': 'myriam',\n",
              " 'airport': 'aéroport',\n",
              " 'sound': 'son',\n",
              " 'status': 'statut',\n",
              " 'irish': 'irlandais',\n",
              " 'placed': 'placé',\n",
              " 'child': 'enfant',\n",
              " 'idea': 'idée',\n",
              " 'foreign': 'étrangères',\n",
              " 'municipality': 'municipalité',\n",
              " 'register': 'registre',\n",
              " 'eight': 'eight',\n",
              " 'problems': 'problèmes',\n",
              " 'native': 'autochtone',\n",
              " 'coverage': 'couverture',\n",
              " 'channel': 'channel',\n",
              " 'parliament': 'parlement',\n",
              " 'username': 'pseudo',\n",
              " 'edition': 'édition',\n",
              " 'minor': 'mineur',\n",
              " 'says': 'dit',\n",
              " 'foundation': 'fondations',\n",
              " 'units': 'unités',\n",
              " 'movie': 'film',\n",
              " 'ice': 'glace',\n",
              " 'simply': 'simplement',\n",
              " 'limited': 'limités',\n",
              " 'unit': 'unit',\n",
              " 'student': 'etudiant',\n",
              " 'previously': 'précédemment',\n",
              " 'stated': 'déclaré',\n",
              " 'governor': 'gouverneur',\n",
              " 'complete': 'complet',\n",
              " 'test': 'tester',\n",
              " 'nominated': 'désignés',\n",
              " 'bill': 'facturer',\n",
              " 'parts': 'parties',\n",
              " 'vocals': 'voix',\n",
              " 'theory': 'théorie',\n",
              " 'regional': 'régional',\n",
              " 'account': 'compte',\n",
              " 'vote': 'voter',\n",
              " 'computer': 'ordinateur',\n",
              " 'none': 'aucune',\n",
              " 'carolina': 'carolina',\n",
              " 'tournament': 'tournoi',\n",
              " 'poland': 'pologne',\n",
              " 'behind': 'derrière',\n",
              " 'wales': 'galles',\n",
              " 'winning': 'gagnant',\n",
              " 'lot': 'lot',\n",
              " 'hospital': 'hôpitaux',\n",
              " 'mid': 'mid',\n",
              " 'taking': 'prenant',\n",
              " 'mountain': 'montagnes',\n",
              " 'higher': 'supérieur',\n",
              " 'cases': 'cas',\n",
              " 'angeles': 'angeles',\n",
              " 'editing': 'édition',\n",
              " 'replaced': 'remplacés',\n",
              " 'food': 'alimentation',\n",
              " 'multiple': 'plusieurs',\n",
              " 'likely': 'probablement',\n",
              " 'terms': 'termes',\n",
              " 'sir': 'monsieur',\n",
              " 'thing': 'chose',\n",
              " 'square': 'carrées',\n",
              " 'try': 'essaye',\n",
              " 'topic': 'sujet',\n",
              " 'woman': 'femme',\n",
              " 'officer': 'officier',\n",
              " 'categories': 'catégories',\n",
              " 'greek': 'grec',\n",
              " 'recent': 'récente',\n",
              " 'sent': 'envoyé',\n",
              " 'copyright': 'copyright',\n",
              " 'speed': 'vitesse',\n",
              " 'templates': 'gabarits',\n",
              " 'money': 'argent',\n",
              " 'saw': 'scie',\n",
              " 'senior': 'senior',\n",
              " 'selected': 'sélectionné',\n",
              " 'introduced': 'introduites',\n",
              " 'politician': 'politicien',\n",
              " 'true': 'véritable',\n",
              " 'required': 'requis',\n",
              " 'regular': 'régulier',\n",
              " 'awarded': 'décernés',\n",
              " 'commercial': 'commerciale',\n",
              " 'cities': 'villes',\n",
              " 'contains': 'contient',\n",
              " 'trade': 'échanges',\n",
              " 'degree': 'degré',\n",
              " 'anti': 'anti',\n",
              " 'birth': 'naissance',\n",
              " 'sun': 'soleil',\n",
              " 'finished': 'terminé',\n",
              " 'rugby': 'rugby',\n",
              " 'earth': 'terre',\n",
              " 'access': 'accéder',\n",
              " 'prior': 'prieur',\n",
              " 'seasons': 'saisons',\n",
              " 'journal': 'journal',\n",
              " 'beginning': 'début',\n",
              " 'software': 'logiciels',\n",
              " 'famous': 'célèbres',\n",
              " 'religious': 'religieuse',\n",
              " 'appear': 'apparaissent',\n",
              " 'martin': 'martin',\n",
              " 'god': 'dieu',\n",
              " 'bit': 'bits',\n",
              " 'hours': 'heures',\n",
              " 'running': 'courir',\n",
              " 'brought': 'amenés',\n",
              " 'missing': 'disparu',\n",
              " 'economic': 'economique',\n",
              " 'structure': 'structure',\n",
              " 'rural': 'rural',\n",
              " 'remained': 'restait',\n",
              " 'decision': 'décision',\n",
              " 'certain': 'certaines',\n",
              " 'hit': 'frapper',\n",
              " 'minutes': 'minutes',\n",
              " 'spain': 'espagne',\n",
              " 'plays': 'joue',\n",
              " 'whole': 'entier',\n",
              " 'joseph': 'joseph',\n",
              " 'lord': 'seigneur',\n",
              " 'web': 'enchaînement',\n",
              " 'decided': 'décidé',\n",
              " 'operations': 'opérations',\n",
              " 'function': 'fonction',\n",
              " 'louis': 'louis',\n",
              " 'assembly': 'assemblage',\n",
              " 'queen': 'queen',\n",
              " 'security': 'sécurité',\n",
              " 'uses': 'utilisations',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92pCNjf40ov1"
      },
      "source": [
        "#### Viendo los diccionarios de Inglés y Francés\n",
        "\n",
        "* `en_fr_train` es un diccionario donde el key es la palabra en inglés y el valor es la palabra en francés.\n",
        "```\n",
        "{'the': 'la',\n",
        " 'and': 'et',\n",
        " 'was': 'était',\n",
        " 'for': 'pour',\n",
        "```\n",
        "\n",
        "* `en_fr_test` es similar que `en_fr_train`, pero este es un conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8HLzSy0ov1"
      },
      "source": [
        "## Generando matrices de embeddings y transformación\n",
        "\n",
        "####  Traducción de diccionario de inglés a francés mediante embeddings\n",
        "\n",
        "Implementaremos la función `get_matrices`, que toma los datos cargados y retorna las matrices `X` y `Y`.\n",
        "\n",
        "Entrada:\n",
        "- `en_fr` : diccionario del Inglés a Francés\n",
        "- `en_embeddings` : embeddings de palabras en inglés\n",
        "- `fr_embeddings` : embeddings de palabras en Francés\n",
        "\n",
        "Retorna:\n",
        "- matrices `X` y `Y`, donde cada renglón en X es la palabra embebida para una plabra en inglés, y lo mismo con Y es la palabra embebida en francés.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\">\n",
        "<img src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/X_to_Y.jpg?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:800px;height:200px;\" /> Figure 2 </div>\n",
        "\n",
        "Utilice el diccionario `en_fr` para asegurarse de que la i-ésima fila de la matriz` X`\n",
        "corresponde a la i-ésima fila de la matriz \"Y\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3C8hvEyp0ov1"
      },
      "outputs": [],
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "  #Crear dos listas X_l, Y_l listas para inglés y francés\n",
        "  X_l = list()\n",
        "  Y_l = list()\n",
        "\n",
        "  #Obtenemos las palabras de inglés\n",
        "  english_set = english_vecs.keys()\n",
        "  french_set = french_vecs.keys()\n",
        "\n",
        "  #Guardar las palabras en francés que son parte del diccionario inglés y francés\n",
        "  french_words = set(en_fr.values())\n",
        "\n",
        "  #Ciclo para las palabras en inglés y francés en el diccionario\n",
        "  for en_word, fr_word in en_fr.items():\n",
        "     #checar qie la palabra en francés tiene un vector embedding y que la palabra en inglés también tenga un vec embedding\n",
        "      if (fr_word in french_set ) and (en_word in english_set):\n",
        "        #obtener los vectores embeddings de la palabra\n",
        "        en_vec = english_vecs[en_word]\n",
        "        fr_vec = french_vecs[fr_word]\n",
        "\n",
        "        #Guardar los embeddings de las palabras en las listas correspondientes a inglés y francés\n",
        "        X_l.append(en_vec)\n",
        "        Y_l.append(fr_vec)\n",
        "    #convertir la lista de vectores embeddings a una matriz\n",
        "  X = np.vstack(X_l)\n",
        "  Y = np.vstack(Y_l)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY8MAMp60ov1"
      },
      "source": [
        "Ahora usaremos la función `get_matrices ()` para obtener los conjuntos `X_train` e` Y_train`\n",
        "de los embeddings de palabras en inglés y francés.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m9pgfNuf0ov1"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(Y_train)"
      ],
      "metadata": {
        "id": "byjoJgum7mMZ",
        "outputId": "ace47fb2-7e06-42c3-bc9f-91c673761617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4932, 4932)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "id": "wdwgr3SZ7rXu",
        "outputId": "9dce507b-1fef-4845-f61c-86512d128c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4932, 300), (4932, 300))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8QtFgb50ov2"
      },
      "source": [
        "\n",
        "# Traductores\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/e_to_f.jpg?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:200px;\" />  </div>\n",
        "\n",
        "Escriba un programa que traduzca palabras del inglés al francés utilizando word embeddings y modelos de espacio vectorial.\n",
        "\n",
        "\n",
        "##  Traducción como transformación lineal de embeddings\n",
        "Dados los diccionarios de embeddings de palabras en inglés y francés, crearemos una matriz de transformación `R`\n",
        "* Dada una palabra incrustada en inglés, $ \\mathbf {e} $, puede multiplicar $ \\mathbf {eR} $ para obtener una nueva palabra incrustada $ \\mathbf {f} $.\n",
        "\n",
        "    \n",
        "* A continuación, puede calcular los vecinos más cercanos a `f` en los embeddings francesas y recomendar la palabra que es más similar a los embeddings de palabras transformadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxaip13q0ov2"
      },
      "source": [
        "### Describir la traducción como el problema de minimización\n",
        "\n",
        "Encuentre una matriz `R` que minimice la siguiente ecuación.\n",
        "\n",
        "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
        "\n",
        "### Norma de Frobenius\n",
        "\n",
        "La norma de Frobenius de una matriz $ A $ (asumiendo que es de dimensión $ m, n $) se define como la raíz cuadrada de la suma de los cuadrados absolutos de sus elementos:\n",
        "\n",
        "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rml0M1Mj0ov2"
      },
      "source": [
        "### Función de perdida\n",
        "\n",
        "En las aplicaciones del mundo real, la pérdida de la norma Frobenius:\n",
        "\n",
        "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
        "\n",
        "a menudo se reemplaza por su valor al cuadrado dividido por $ m $:\n",
        "\n",
        "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
        "\n",
        "donde $ m $ es el número de ejemplos (filas en $ \\mathbf {X} $).\n",
        "\n",
        "* Se encuentra la misma R cuando se usa esta función de pérdida en comparación con la norma de Frobenius original.\n",
        "* La razón para tomar el cuadrado es que es más fácil calcular el gradiente del Frobenius al cuadrado.\n",
        "* La razón para dividir entre $ m $ es que estamos más interesados en la pérdida promedio por inserción que en la pérdida de todo el conjunto de entrenamiento.\n",
        "     * La pérdida de todo el conjunto de entrenamiento aumenta con más palabras (ejemplos de entrenamiento),\n",
        "     por lo que tomar el promedio nos ayuda a rastrear la pérdida promedio independientemente del tamaño del conjunto de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJrlJ_I30ov2"
      },
      "source": [
        "\n",
        "### Implementación del mecanismo de traducción\n",
        "\n",
        "#### Calculando el loss\n",
        "* La función de pérdida será la norma de Frobenoius al cuadrado de la diferencia entre\n",
        "matriz y su aproximación, dividida por el número de ejemplos de entrenamiento $ m $.\n",
        "* Su fórmula es:\n",
        "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
        "\n",
        "donde $a_{i j}$ es el valo de la $i$-ésimo renglón y $j$-ésima columna de la matriz $\\mathbf{XR}-\\mathbf{Y}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUcnWdut0ov2"
      },
      "source": [
        "#### crear la función `compute_loss()`\n",
        "* Calcular la aproximación de `Y` mediante la matriz multiplicando` X` y `R`\n",
        "* Calcular la diferencia `XR - Y`\n",
        "* Calcular la norma de Frobenius al cuadrado de la diferencia y divídala por $ m $.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "61LUdETh0ov2"
      },
      "outputs": [],
      "source": [
        "def compute_loss(X,Y,R):\n",
        "  m = X.shape[0]\n",
        "  loss = np.dot(X,R) - Y\n",
        "  loss2 = loss**2\n",
        "  sum_loss2 = np.sum(loss2)\n",
        "\n",
        "  L = sum_loss2/m\n",
        "\n",
        "  return L\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dnJYXNL0ov2"
      },
      "source": [
        "\n",
        "### Calculando el gradiente de la función loss con respecto a la matriz de transforación R\n",
        "\n",
        "* Calcular el gradiente de la pérdida con respecto a la matriz de transformación \"R\".\n",
        "* El gradiente nos da la dirección en la que debemos disminuir `R`\n",
        "para minimizar la pérdida.\n",
        "* $ m $ es el número de ejemplos de entrenamiento (número de filas en $ X $).\n",
        "* La fórmula para el gradiente de la función de pérdida $ 𝐿 (𝑋, 𝑌, 𝑅) $ es:\n",
        "\n",
        "$$\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6__LcQQc0ov2"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(X,Y,R):\n",
        "  m=X.shape[0]\n",
        "  gradient = np.dot(X.transpose(), np.dot(X,R)-Y)*(2/m)\n",
        "  return gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSGJmqj80ov3"
      },
      "source": [
        "### Encontrar la R óptima con el algoritmo de descenso de gradiente\n",
        "\n",
        "#### Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXY0_h_0ov3"
      },
      "source": [
        "Pseudocódigo:\n",
        "1. Calcular el gradiente $g$ del loss con respecto a la matriz $R$.\n",
        "2. Update $R$ con la formula:\n",
        "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
        "\n",
        "Donde $\\alpha$ es el learning rate, que es un escalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc8V4ljA0ov3"
      },
      "source": [
        "#### Learning rate\n",
        "\n",
        "* La tasa de aprendizaje o \"tamaño de paso\" $ \\ alpha $ es un coeficiente que decide cuánto queremos cambiar $ R $ en cada paso.\n",
        "* Si cambiamos $ R $ demasiado, podríamos saltarnos el óptimo dando un paso demasiado grande.\n",
        "* Si solo hacemos pequeños cambios en $ R $, necesitaremos muchos pasos para alcanzar el óptimo.\n",
        "* La tasa de aprendizaje $ \\ alpha $ se usa para controlar esos cambios.\n",
        "* Los valores de $ \\ alpha $ se eligen dependiendo del problema, y usaremos `learning_rate` $ = 0.0003 $ como valor predeterminado para nuestro algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "BMSyheS_BEVX",
        "outputId": "43dca180-5d82-4ff7-8ea5-bf2ad5464087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4932, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABMeSZzR0ov3"
      },
      "source": [
        "#### Implementar la función `align_embeddings()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mvtmX7ca0ov3"
      },
      "outputs": [],
      "source": [
        "def align_embeddings(X,Y, train_steps=100, learning_rate=0.0003):\n",
        "  np.random.seed(0)\n",
        "  R = np.random.rand(X.shape[1], X.shape[1]) #Matriz cuadrada de 300x300\n",
        "\n",
        "  for i in range(train_steps):\n",
        "    if i%25==0:\n",
        "      print(f'La pérdida en la iteración {i} es {compute_loss(X,Y,R):.4f}')\n",
        "    gradiente = compute_gradient(X,Y,R)\n",
        "\n",
        "    #Actualizar R\n",
        "    R = R - learning_rate*gradiente\n",
        "\n",
        "  return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rQl1bqEl0ov3",
        "outputId": "10c15e98-533d-444f-cabb-8898f94df7d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La pérdida en la iteración 0 es 4.5591\n",
            "La pérdida en la iteración 25 es 4.4492\n",
            "La pérdida en la iteración 50 es 4.3421\n",
            "La pérdida en la iteración 75 es 4.2379\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(129)\n",
        "m = 10\n",
        "n = 5\n",
        "X = np.random.rand(m,n)\n",
        "Y = np.random.rand(m,n)\n",
        "R = align_embeddings(X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "R.shape"
      ],
      "metadata": {
        "id": "MnMiGiOKCI2-",
        "outputId": "80d116f0-86d8-4b35-f678-27f51b843a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewt1-OGP0ov3"
      },
      "source": [
        "## Calcular la matriz de transformación R\n",
        "\n",
        "Usando el conjunto de entrenamiento, encuentre la matriz de transformación $\\mathbf{R}$ llamando a la función `align_embeddings()`.\n",
        "\n",
        "**NOTA:** La siguiente celda de código tardará unos minutos en ejecutarse por completo (~3 minutos)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "oCkqh4ClCrto",
        "outputId": "d9013d0d-c7fd-4a7d-d8e9-c65502148f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4932, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zUkvkD0M0ov3",
        "outputId": "8f2e593a-4ba1-4e2c-8295-9951633116f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La pérdida en la iteración 0 es 957.9498\n",
            "La pérdida en la iteración 25 es 97.4537\n",
            "La pérdida en la iteración 50 es 26.7613\n",
            "La pérdida en la iteración 75 es 9.7934\n",
            "La pérdida en la iteración 100 es 4.3927\n",
            "La pérdida en la iteración 125 es 2.3407\n",
            "La pérdida en la iteración 150 es 1.4567\n",
            "La pérdida en la iteración 175 es 1.0395\n",
            "La pérdida en la iteración 200 es 0.8288\n",
            "La pérdida en la iteración 225 es 0.7169\n",
            "La pérdida en la iteración 250 es 0.6549\n",
            "La pérdida en la iteración 275 es 0.6195\n",
            "La pérdida en la iteración 300 es 0.5987\n",
            "La pérdida en la iteración 325 es 0.5862\n",
            "La pérdida en la iteración 350 es 0.5785\n",
            "La pérdida en la iteración 375 es 0.5737\n",
            "La pérdida en la iteración 400 es 0.5707\n",
            "La pérdida en la iteración 425 es 0.5687\n",
            "La pérdida en la iteración 450 es 0.5674\n",
            "La pérdida en la iteración 475 es 0.5665\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01094974, -0.01264808,  0.00060607, ...,  0.00090619,\n",
              "        -0.00317652,  0.00120469],\n",
              "       [ 0.01800591,  0.00169838,  0.0030341 , ...,  0.02487439,\n",
              "        -0.00213227,  0.00160555],\n",
              "       [ 0.00575546,  0.00059365,  0.01488434, ..., -0.00164987,\n",
              "        -0.00653721,  0.00537056],\n",
              "       ...,\n",
              "       [-0.00174451,  0.00417253, -0.01830601, ..., -0.00990938,\n",
              "         0.00581949,  0.00226011],\n",
              "       [ 0.00442384, -0.00485468, -0.00123269, ..., -0.00439163,\n",
              "        -0.00102191,  0.0011057 ],\n",
              "       [ 0.00557851, -0.01249637, -0.00374668, ..., -0.01588083,\n",
              "         0.00900329, -0.00426499]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "align_embeddings(X_train,Y_train,train_steps=500, learning_rate=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLaBOuuk0ov3"
      },
      "source": [
        "\n",
        "## Probando el traductor\n",
        "\n",
        "### Algoritmo k-Nearest neighbors\n",
        "\n",
        "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
        "* k-NN es un método que toma un vector como entrada y encuentra los otros vectores en el conjunto de datos que están más cerca de él.\n",
        "* La 'k' es el número de \"vecinos más cercanos\" a encontrar (por ejemplo, k = 2 encuentra los dos vecinos más cercanos).\n",
        "\n",
        "### Buscando el embedding de la traducción\n",
        "Dado que estamos aproximando la función de traducción de los embeddings de inglés a francés mediante una matriz de transformación lineal $ \\mathbf {R} $, la mayoría de las veces no obtendremos la incrustación exacta de una palabra francesa cuando transformamos los embeddings $ \\mathbf { e} $ de alguna palabra en inglés en particular en el espacio de embeddings francés.\n",
        "* ¡Aquí es donde $ k $ -NN se vuelve realmente útil! Al usar $ 1 $ -NN con $ \\mathbf {eR} $ como entrada, podemos buscar un embedding $ \\mathbf {f} $ (como una fila) en la matriz $ \\mathbf {Y} $ que es la más cercana a el vector transformado $ \\mathbf {eR} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwrDaiBx0ov3"
      },
      "source": [
        "### Similaridad por  Coseno\n",
        "Similitud de coseno entre los vectores $ u $ y $ v $ calculada como el coseno del ángulo entre ellos.\n",
        "La formula es\n",
        "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
        "\n",
        "* $\\cos(u,v)$ = $1$ cuando $u$ y $v$ se encuentran en la misma línea y tienen la misma dirección.\n",
        "* $\\cos(u,v)$ es $-1$ Cuando ellas tienen direcciones exactamente opuestas.\n",
        "* $\\cos(u,v)$ es $0$ cuando los vectores son ortogonales (perpendiculares) entre sí."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkzrRi3H0ov4"
      },
      "source": [
        "#### Nota: La distancia y la similitud son cosas bastante opuestas.\n",
        "* Podemos obtener la métrica de la distancia a partir de la similitud del coseno, pero la similitud del coseno no se puede usar directamente como la métrica de la distancia.\n",
        "* Cuando la similitud del coseno aumenta (hacia $ 1 $), la \"distancia\" entre los dos vectores disminuye (hacia $ 0 $).\n",
        "* Podemos definir la distancia del coseno entre $ u $ y $ v $ como\n",
        "$$ d_{\\text {cos}} (u, v) = 1- \\cos(u, v) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yde-g_2I0ov4"
      },
      "outputs": [],
      "source": [
        "#cosine_similarity\n",
        "def cosine_similarity(A,B):\n",
        "  dot = np.dot(A,B)\n",
        "  norma = np.linalg.norm(A)\n",
        "  normb = np.linalg.norm(B)\n",
        "  cos = dot/(norma*normb)\n",
        "  return cos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcxAjAW0ov4"
      },
      "source": [
        "**Crear la función**`nearest_neighbor()`\n",
        "\n",
        "Inputs:\n",
        "* Vector `v`,\n",
        "* Conjunto de posibles vecinos `candidates`\n",
        "* `k` vecinos a encontrar.\n",
        "* La métrica de distancia a utilizar (sim coseno).\n",
        "* `cosine_similarity` La función de similaridad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LWDl4IGh0ov4"
      },
      "outputs": [],
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "  similarity_l = []\n",
        "  for row in candidates:\n",
        "    cos_similarity = cosine_similarity(v,row)\n",
        "    similarity_l.append(cos_similarity)\n",
        "  sorted_ids = np.argsort(similarity_l)\n",
        "\n",
        "  k_idx = sorted_ids[-k:]\n",
        "\n",
        "  return k_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mKo1XCyO0ov8",
        "outputId": "2c215cc3-2f99-45b7-f212-e24e6284527c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2]\n"
          ]
        }
      ],
      "source": [
        "# Test de la implementación:\n",
        "v = np.array([1,0,1])\n",
        "candidates = np.array([[1,0,5],[-2,5,3], [2,0,1], [6,-9,5], [9,9,9]])\n",
        "print(nearest_neighbor(v, candidates, k=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidates"
      ],
      "metadata": {
        "id": "AiA58sG3Hitd",
        "outputId": "deae1cd5-cde4-4ac7-d88a-128c6e27db2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  0,  5],\n",
              "       [-2,  5,  3],\n",
              "       [ 2,  0,  1],\n",
              "       [ 6, -9,  5],\n",
              "       [ 9,  9,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidates[nearest_neighbor(v, candidates, k=3)]"
      ],
      "metadata": {
        "id": "w5PnLEWJHMcj",
        "outputId": "9b111f39-5230-4b2a-cc10-6d1624642956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9, 9, 9],\n",
              "       [1, 0, 5],\n",
              "       [2, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkB_90gf0ov8"
      },
      "source": [
        "### Porbando la traducción y calculando el accuracy\n",
        "\n",
        "Crearemos la función `test_vocabulary` que mapea en inglés la matriz $X$, matriz $Y$ y $R$ y devuelve el accuracy de las traducciones de $X$ a $Y$ por $R$.\n",
        "\n",
        "* Calculando el accuracy como $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fohlnhu70ov8"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW_h3JTS0ov9"
      },
      "source": [
        "Veamos cómo funciona su mecanismo de traducción con los datos invisibles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Esbr03k40ov9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25DNEmGr0ov9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJpuv4Df0ov9"
      },
      "source": [
        "Logramos traducir palabras de un idioma a otro.\n",
        "sin siquiera verlos con casi un 56% de precisión usando algunos conceptos básicos como:\n",
        "¡álgebra lineal y aprender a mapear palabras de un idioma a otro!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgr_5YlE0ov9"
      },
      "source": [
        "\n",
        "# LSH y búsqueda de documentos\n",
        "\n",
        "* Procesar los tweets y representar cada tweet como un vector (representar un\n",
        "documento con un embedding vector).\n",
        "* Utilice hashing y k vecinos más cercanos para encontrar tweets\n",
        "que son similares a un tweet determinado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz9Jg4QF0ov9"
      },
      "outputs": [],
      "source": [
        "# tweets positivos y negativos\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "all_tweets = all_positive_tweets + all_negative_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcK0Pg4P0ov9"
      },
      "source": [
        "\n",
        "###  Obteniendo los embeddings de los documentos\n",
        "\n",
        "#### Modelo de Bag-of-words (BOW)\n",
        "\n",
        "Los documentos de texto son secuencias de palabras.\n",
        "* El orden de las palabras marca la diferencia. Por ejemplo, las frases \"La tarta de manzana es mejor que la pizza de pepperoni \"y\" La pizza de pepperoni es mejor que la tarta de manzana \"\n",
        "tienen significados opuestos debido al orden de las palabras.\n",
        "* Sin embargo, para algunas aplicaciones, ignorar el orden de las palabras puede permitir\n",
        "nosotros para formar un modelo eficiente y aún eficaz.\n",
        "* Este enfoque se denomina modelo de documento de bolsa de palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV6-Xasi0ov9"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2TnhZAh0ov-"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "            word not in string.punctuation):\n",
        "            stem_word = stemmer.stem(word)\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUSQCrDi0ov-"
      },
      "source": [
        "\n",
        "Crear la función `get_document_embedding()`.\n",
        "* La función `get_document_embedding()` codifica el documento completo como un embedding del \"documento\".\n",
        "* Tomar un documento (como una cadena) y un diccionario, `en_embeddings`\n",
        "* Procesar el documento y buscar el embedding correspondiente de cada palabra.\n",
        "* Luego sumar y devuelver la suma de todos los vectores de palabras de ese tweet procesado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT_NeASd0ov-"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqjgSitA0ov-"
      },
      "outputs": [],
      "source": [
        "# testear la función\n",
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDRo6wyc0ov-"
      },
      "source": [
        "#### Guardar los vectores de documento en un diccionario `get_document_vecs()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVMMQcxM0ov_"
      },
      "outputs": [],
      "source": [
        "# UNQ_C14 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def get_document_vecs(all_docs, en_embeddings):\n",
        "\n",
        "    return document_vec_matrix, ind2Doc_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh-GJN-70ov_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1EzFxWF0ov_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxYpcc280ov_"
      },
      "source": [
        "\n",
        "## Buscando los tweets\n",
        "\n",
        "Ahora tenemos un vector de dimensión (m, d) donde `m` es el número de tweets\n",
        "(10,000) y `d` es la dimensión de los embeddings (300)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hspZpw7Q0owA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4l43a4j0owA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvoIkSK50owA"
      },
      "source": [
        "\n",
        "## Buscando los tweets más similares con LSH\n",
        "\n",
        "Ahora implementaremos el hashing (LSH) para identificar el tweet más similar.\n",
        "* En lugar de mirar los 10,000 vectores, puede buscar un subconjunto para encontrar\n",
        "sus vecinos más cercanos.\n",
        "\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/one.png?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 3 </div>\n",
        "\n",
        "Puede dividir el espacio vectorial en regiones y buscar dentro de una región los vecinos más cercanos de un vector dado.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/four.png?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 4 </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvWod2-G0owA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0JFWO5C0owA"
      },
      "source": [
        "#### Escogiendo los número de planos\n",
        "\n",
        "* Cada plano divide el espacio en $ 2 $ partes.\n",
        "* Entonces $ n $ planos dividen el espacio en $ 2 ^ {n} $ hash.\n",
        "* Queremos organizar 10,000 vectores de documentos en depósitos para que cada depósito tenga vectores de $ ~ 16 $.\n",
        "* Para eso necesitamos $ \\frac {10000} {16} = 625 $ buckets.\n",
        "* Estamos interesados en $ n $, número de aviones, por lo que $ 2 ^ {n} = 625 $. Ahora, podemos calcular $ n = \\log_{2} 625 = 9.29 \\approx 10 $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBRdhjIC0owA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ZfbT-N0owA"
      },
      "source": [
        "## Obteniendo el número para el vector hash\n",
        "\n",
        "Para cada vector, necesitamos obtener un número único asociado a ese vector para poder asignarlo a un \"bucket de hash\".\n",
        "\n",
        "### Hyperlanes in vector spaces\n",
        "* En un espacio vectorial dimensional de $ 3 $, el hiperplano es un plano regular. En el espacio vectorial dimensional de $ 2 $, el hiperplano es una línea.\n",
        "* Generalmente, el hiperplano es un subespacio que tiene una dimensión $ 1 $ menor que la del espacio vectorial original.\n",
        "* Un hiperplano se define de forma única por su vector normal.\n",
        "* El vector normal $ n $ del plano $ \\ pi $ es el vector al que todos los vectores en el plano $ \\ pi $ son ortogonales (perpendiculares en el caso dimensional de $ 3 $).\n",
        "\n",
        "### Usando Hiperplanos para cortar el espacio vectorial\n",
        "\n",
        "Podemos usar un hiperplano para dividir el espacio vectorial en $ 2 $ partes.\n",
        "* Todos los vectores cuyo producto escalar con el vector normal de un plano es positivo están en un lado del plano.\n",
        "* Todos los vectores cuyo producto escalar con el vector normal del plano es negativo están en el otro lado del plano.\n",
        "\n",
        "### Encodeando los buckets hash\n",
        "* Para un vector, podemos tomar su producto escalar con todos los planos, luego codificar esta información para asignar el vector a un solo cubo hash.\n",
        "* Cuando el vector apunta al lado opuesto del hiperplano de lo normal, codifíquelo con 0.\n",
        "* De lo contrario, si el vector está en el mismo lado que el vector normal, codifíquelo por 1.\n",
        "* Si calcula el producto escalar con cada plano en el mismo orden para cada vector, ha codificado el ID de hash único de cada vector como un número binario, como [0, 1, 1, ... 0].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Knx2db0owB"
      },
      "source": [
        "## Obteniendo el número para el vector hash\n",
        "\n",
        "Para cada vector, necesitamos obtener un número único asociado a ese vector para poder asignarlo a un \"bucket de hash\".\n",
        "\n",
        "### Hiperplanos en espacios de vectores\n",
        "* En un espacio vectorial dimensional de $ 3 $, el hiperplano es un plano regular. En el espacio vectorial dimensional de $ 2 $, el hiperplano es una línea.\n",
        "* Generalmente, el hiperplano es un subespacio que tiene una dimensión $ 1 $ menor que la del espacio vectorial original.\n",
        "* Un hiperplano se define de forma única por su vector normal.\n",
        "* El vector normal $ n $ del plano $ \\ pi $ es el vector al que todos los vectores en el plano $ \\ pi $ son ortogonales (perpendiculares en el caso dimensional de $ 3 $).\n",
        "\n",
        "### Usando Hiperplanos para cortar el espacio vectorial\n",
        "\n",
        "Podemos usar un hiperplano para dividir el espacio vectorial en $ 2 $ partes.\n",
        "* Todos los vectores cuyo producto escalar con el vector normal de un plano es positivo están en un lado del plano.\n",
        "* Todos los vectores cuyo producto escalar con el vector normal del plano es negativo están en el otro lado del plano.\n",
        "\n",
        "### Encodeando los buckets hash\n",
        "* Para un vector, podemos tomar su producto escalar con todos los planos, luego codificar esta información para asignar el vector a un solo cubo hash.\n",
        "* Cuando el vector apunta al lado opuesto del hiperplano de lo normal, codifíquelo con 0.\n",
        "* De lo contrario, si el vector está en el mismo lado que el vector normal, codifíquelo por 1.\n",
        "* Si calcula el producto escalar con cada plano en el mismo orden para cada vector, ha codificado el ID de hash único de cada vector como un número binario, como [0, 1, 1, ... 0].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk1IGNhb0owB"
      },
      "source": [
        "### Implementando los hash buckets\n",
        "\n",
        "Inicializamos los hash de la tabla. Es una lista de matrices `N_UNIVERSES`, cada una describe su propia tabla hash. Cada matriz tiene filas `N_DIMS` y columnas` N_PLANES`. Cada columna de esa matriz es un vector normal dimensional `N_DIMS` para cada uno de los hiperplanos` N_PLANES` que se utilizan para crear cubos de la tabla hash particular.\n",
        "\n",
        "* Primero multiplica tu vector `v`, con un plano correspondiente. Esto le dará un vector de dimensión $ (1, \\text{N_planes}) $.\n",
        "* Luego, convertirá todos los elementos de ese vector en 0 o 1.\n",
        "* Creas un vector hash haciendo lo siguiente: si el elemento es negativo, se convierte en un 0; de lo contrario, lo cambias a un 1.\n",
        "* Luego calcula el número único para el vector iterando sobre `N_PLANES`\n",
        "* Luego multiplica $ 2^i $ por el bit correspondiente (0 o 1).\n",
        "* Luego almacenará esa suma en la variable `hash_value`.\n",
        "\n",
        "Cree un hash para el vector en la siguiente función.\n",
        "Utilice esta fórmula:\n",
        "\n",
        "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDAXjgY0owB"
      },
      "source": [
        "#### Crea los conjuntos de planos\n",
        "* Cree múltiples (25) conjuntos de planos (los planos que dividen la región).\n",
        "* Puede pensar en estos como 25 formas distintas de dividir el espacio vectorial con un conjunto diferente de planos.\n",
        "* Cada elemento de esta lista contiene una matriz con 300 filas (la palabra vector tiene 300 dimensiones) y 10 columnas (hay 10 planos en cada \"universo\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YImzhG4c0owC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ModPaW1g0owC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC2oQdJN0owC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tIqvEqK0owC"
      },
      "source": [
        "\n",
        "## Creando la Tabla Hash\n",
        "\n",
        "Dado que ya tenemos una representación para cada vector (o tweet), ahora crearemos una tabla hash. Necesitamos una tabla hash, de modo que, dado un hash_id, pueda buscar rápidamente los vectores correspondientes. Esto le permite reducir su búsqueda por una cantidad de tiempo significativa.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/gdesirena/Procesamiento_Natural_del_Lenguaje_2024/blob/main/Modulo%20III/Figures/table.png?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px;\" />  </div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYpjQ6Up0owC"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MJTRg0Y0owC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4STtlUz0owC"
      },
      "source": [
        "<a name=\"3-6\"></a>\n",
        "\n",
        "### Creando todas las tablas hash\n",
        "\n",
        "Ahora podemos aplicar la función hash a los vectores y almacenarlos en una tabla hash que\n",
        "le permitiría buscar rápidamente vectores similares.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWwuO_wW0owC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAzWOOv50owC"
      },
      "source": [
        "### Aproximar K-NN\n",
        "\n",
        "\n",
        "Implementar aproximadamente K vecinos más cercanos utilizando hash sensible a la localidad,\n",
        "para buscar documentos que sean similares a un documento dado en el\n",
        "índice `doc_id`.\n",
        "\n",
        "##### entradas\n",
        "* `doc_id` es el índice de la lista de documentos` all_tweets`.\n",
        "* `v` es el vector de documento para el tweet en` all_tweets` en el índice `doc_id`.\n",
        "* `planes_l` es la lista de planos (la variable global creada anteriormente).\n",
        "* `k` es el número de vecinos más cercanos a buscar.\n",
        "* `num_universes_to_use`: para ahorrar tiempo, podemos usar menos que el total\n",
        "número de universos disponibles. De forma predeterminada, está configurado en `N_UNIVERSES`,\n",
        "que es $ 25 $ para esta asignación.\n",
        "\n",
        "La función `approx_knn` encuentra un subconjunto de vectores candidatos que\n",
        "están en el mismo \"depósito de hash\" que el vector de entrada 'v'. Entonces se realiza\n",
        "los k vecinos más cercanos habituales buscan en este subconjunto (en lugar de buscar\n",
        "a través de los 10,000 tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otpd-Yzw0owD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2MOG4y90owD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usevpxJD0owD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OP2hXau0owD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC1-4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}